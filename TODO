---+ Document hookable rdf_load/2

---+ Concurrent version of RDF-DB.

---++ Objectives

  * Provide concurrency similar to Prolog's dynamic DB, using the Prolog update
    semantics.  Steps:

    1. Avoid the _need_ for reindexing
    2. Introduce Logical update semantics for RDF triples
    3. Allow concurrent read and write

---++++ Dealing with predicate clouds:

  * Have the link on predicates + generation and have immutable clouds
    - Array of generation-ranges to clouds
      - Start searching this from the latest
      - Remove on GC
      - If predicate-hash must change at generation X, copy triples
    - Adding/deleting a triple inside a cloud:
      1. Create a copy
      2. Set new one from current generation on all predicates
    - Adding a triples that joins two clouds
      2. Create new cloud with all triples and hash of `largest'
      3. Copy triples to new hash
      4. Set new one from current generation on all predicates.
    - Delete a triples that splits a cloud
      1. Create two new clouds.  Largest takes hash of old.
      2. Copy triples of smaller to hash of new cloud.
      3. Set new clouds on current generation on all predicates.

---+++ Transaction semantics

  * All goals in a transaction see the database at the generation of the
  transaction start.
  * All goals in a _nested_ transaction see changes done by the parent
  transaction upto the start of the nested transaction.
  * Update semantics
    - A transaction maintains a list of affected triples.
    - A transaction is assigned a transaction generation, which count
      from near the maximum generation.
      - Each added triple is born at the transaction-generation
      - Each triples deleted died at the transaction-generation
    - If a transaction is _committed_ we scan the affected triples and
      - Each added triple is born at the current generation
      - Each deleted triple died at the current generation
    - If a transaction is _rolled_back_
      - Each added triple is born at the max generation
	- GC will take care of them
      - Each deleted triple died at the max generation (as initial)

  * Transaction generation
    - Must be far enough away from `now'; allocated near the end.
    - All triples in a transaction are from the same generation.
    - Nested transactions need a generation > parent
    - Multiple threads may run multiple transactions concurrently.
      - Groups: max-threads, max-nested?  Ok: 1000*1000 = 1m is still
      nothing.
      - Outer transaction visible: generation of start
      - Inner transaction visible: generation of start of outer +
        generations of parent transactions.

---++ Non-blocking GC

  * `non-blocking' GC can run in a separate thread!:
    - Keep track of `oldest' generation
      - open_query()
      - close_query()
    - Remove older triples from links without locking.  Leave
      unlinked triples to Boehm-GC

---+ Adding triples

  - LOCKED
    1. Obtain a generation.
    2. Add the triples.
    3. Step the generation to the newly obtained generation.

  The lock must be held as short as possible.  What about this:

  - add_triples(triple *triples, size_t count)
    1. Adds triples to the chains, setting generation to the future.
    2. Do the above, where (2) sets the proper generation.
		  -- also requires a lock :-(


---+ Deleting triples

  - Set deleted generation to current; increment current generation.
  - GC does the remainder of the admin updates.

---+ Duplicates:

  - Test for duplicates; set flag on all of them.			[OK]
  - Avoid duplicate answers by maintaining a table, only adding
    triples that are flagged as duplicates.
  - Leave resetting the duplicate flag to GC:
    - If a triple marked as duplicate is erased, check
      whether alternatives remain in the DB.

---+ Hash-table design

  * Resize
    - Double buckets using lock-free dynamic arrays!
    - Lookup using K%N, K%(2N), ... (is like Cuckoo hashing)
	- Needs to know N0.  Do not make N0 too small.
    - Unbalance: optimize by copying triples				[OK]
    - How to estimate complexity based on hash?
	- Get sizes of different candidates and do the math (TBD)

---+ References

Lock-free library:	http://www.liblfds.org/
Useful stuff:		http://eternallyconfuzzled.com/tuts/datastructures/

